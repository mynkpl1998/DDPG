{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0072cc01-2149-44cc-b604-23febf942d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f72707d12f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "seed_value = 0\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70788e59-51fb-49c3-8fd4-5b6c92c59b54",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a23f5553-4293-4cfd-ab7e-8f07da7cf762",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c65eb-d5d8-46d4-9113-c4f703a15beb",
   "metadata": {},
   "source": [
    "## Tensorboard Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6b6fc3c-49de-416a-bd79-6f2f18a276d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = {\n",
    "    'replay-size': 50000,\n",
    "    'polyak': 0.995,\n",
    "    'hidden_size': 256,\n",
    "    'mini_batch_size': 256,\n",
    "    'update_frequency_iters': 500,\n",
    "    'num_update_iters': 10,\n",
    "    'gamma': 0.9,\n",
    "    'q_network_lr':1e-5,\n",
    "    'policy_network_lr': 1e-3,\n",
    "    'num_training_episodes': 50000,\n",
    "    'explore_noise_scale_start': 0.5,\n",
    "    'explore_noise_scale_final': 0.001,\n",
    "    'num_test_episodes': 20,\n",
    "}\n",
    "\n",
    "writer = SummaryWriter('runs/ddpg/Pendulum_update_freq_500')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7e399-5968-473c-a9ad-1a29036f94c3",
   "metadata": {},
   "source": [
    "## Experience Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea6283b-8802-4c86-b2e8-5dd2c82d7ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, maxsize):\n",
    "        self.__maxsize = maxsize\n",
    "        self.__buffer = deque(maxlen=maxsize)\n",
    "\n",
    "    @property\n",
    "    def maxsize(self):\n",
    "        return self.__maxsize\n",
    "\n",
    "    @property\n",
    "    def replay_size(self):\n",
    "        return len(self.__buffer)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.__buffer))\n",
    "        return batch_size, random.sample(self.__buffer, batch_size)\n",
    "    \n",
    "    def add(self, tup):\n",
    "        self.__buffer.append(tup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae78da-14ef-48e6-b13b-5ba5d92c89ee",
   "metadata": {},
   "source": [
    "## Q-value approxmiator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "131a46bf-f256-4ba9-9368-049bfab754f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 observation_dims: int,\n",
    "                 action_dims:int,\n",
    "                 hidden_size:int = 128):\n",
    "\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.__observation_dims = observation_dims\n",
    "        self.__action_dims = action_dims\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(out_features=hidden_size, in_features=observation_dims + action_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_features=hidden_size, in_features=hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_features=1, in_features=hidden_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, \n",
    "                state,\n",
    "                action):\n",
    "        x = torch.cat((state, action), 1)\n",
    "        return self.value(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe3cac9-4243-4ebf-8620-ae09b91db93b",
   "metadata": {},
   "source": [
    "## Policy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2b3e8c7-8f36-4a92-bed9-eccf13f71950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 observation_dims: int,\n",
    "                 action_dims:int,\n",
    "                 hidden_size:int = 128):\n",
    "\n",
    "        super(Policy, self).__init__()\n",
    "        self.__observation_dims = observation_dims\n",
    "        self.__action_dims = action_dims\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(out_features=hidden_size, in_features=observation_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_features=hidden_size, in_features=hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_features=action_dims, in_features=hidden_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, \n",
    "                x):\n",
    "        return self.value(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aed898-d5b1-42c6-b861-8272067c5613",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a9d4aee-8fa0-443b-b0bd-9c64d4d8750b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–‹                                    | 979/50000 [06:36<5:30:54,  2.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 256\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__tensorboard_writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Episode Reward\u001b[39m\u001b[38;5;124m'\u001b[39m, test_cum_reward\u001b[38;5;241m.\u001b[39mmean(), episode)\n\u001b[1;32m    247\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(env,\n\u001b[1;32m    248\u001b[0m                   hyper_parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplay-size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    249\u001b[0m                   hyper_parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m                   hyper_parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolyak\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    254\u001b[0m                   writer)\n\u001b[0;32m--> 256\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_training_episodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m              \u001b[49m\u001b[43mnoise_scale_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexplore_noise_scale_start\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m              \u001b[49m\u001b[43mnoise_scale_final\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexplore_noise_scale_final\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m              \u001b[49m\u001b[43mminibatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmini_batch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m              \u001b[49m\u001b[43mweight_update_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mupdate_frequency_iters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m              \u001b[49m\u001b[43mweight_update_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_update_iters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m              \u001b[49m\u001b[43mnum_test_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyper_parameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_test_episodes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 222\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, train_episodes, noise_scale_start, noise_scale_final, minibatch_size, weight_update_frequency, weight_update_iters, num_test_episodes)\u001b[0m\n\u001b[1;32m    219\u001b[0m policy_losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _iter_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, weight_update_iters):\n\u001b[0;32m--> 222\u001b[0m     q_loss, policy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminibatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_iters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     q_losses\u001b[38;5;241m.\u001b[39mappend(q_loss)\n\u001b[1;32m    224\u001b[0m     policy_losses\u001b[38;5;241m.\u001b[39mappend(policy_loss)\n",
      "Cell \u001b[0;32mIn[8], line 136\u001b[0m, in \u001b[0;36mTrainer.update_network\u001b[0;34m(self, batch_size, epoch)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__policy_phi\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m--> 136\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__tensorboard_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_histogram\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactor/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/gradient\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Softly move the weights of target network\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, target_param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__q_phi\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__q_tar\u001b[38;5;241m.\u001b[39mparameters()):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 replay_size: int,\n",
    "                 hidden_size: int,\n",
    "                 gamma: int,\n",
    "                 q_network_lr: float,\n",
    "                 policy_network_lr: float,\n",
    "                 polyak: float,\n",
    "                 tensorboard_writer):\n",
    "\n",
    "        self.__env = env\n",
    "        self.__replay_size = replay_size\n",
    "        self.__gamma = gamma\n",
    "        self.__polyak = polyak\n",
    "        self.__tensorboard_writer = tensorboard_writer\n",
    "\n",
    "        # Create a experience replay\n",
    "        self.__exp_replay = ReplayBuffer(maxsize=replay_size)\n",
    "\n",
    "        # Create Q networks\n",
    "        self.__q_phi = QNetwork(observation_dims=env.observation_space.shape[0],\n",
    "                                action_dims=env.action_space.shape[0],\n",
    "                                hidden_size=hidden_size)\n",
    "        \n",
    "        self.__q_tar = QNetwork(observation_dims=env.observation_space.shape[0],\n",
    "                                action_dims=env.action_space.shape[0],\n",
    "                                hidden_size=hidden_size)\n",
    "        \n",
    "        self.__q_phi.load_state_dict(self.__q_tar.state_dict())\n",
    "\n",
    "        # Create Policy Networks\n",
    "        self.__policy_phi = Policy(observation_dims=env.observation_space.shape[0],\n",
    "                                   action_dims=env.action_space.shape[0],\n",
    "                                   hidden_size=hidden_size)\n",
    "\n",
    "        self.__policy_tar = Policy(observation_dims=env.observation_space.shape[0],\n",
    "                                   action_dims=env.action_space.shape[0],\n",
    "                                   hidden_size=hidden_size)\n",
    "        self.__policy_phi.load_state_dict(self.__policy_tar.state_dict())\n",
    "\n",
    "        # Create loss function for q network\n",
    "        self.__q_loss_func = nn.MSELoss(reduction='mean')\n",
    "        self.__q_optimizer = optim.Adam(self.__q_phi.parameters(), lr=q_network_lr)\n",
    "\n",
    "        # Create loss function for policy network\n",
    "        self.__policy_optimizer = optim.Adam(self.__policy_phi.parameters(), lr=policy_network_lr)\n",
    "        \n",
    "\n",
    "    def get_action(self, \n",
    "                   state,\n",
    "                   noise: float=0.0):\n",
    "        torch_state = torch.from_numpy(state)\n",
    "        low = self.__env.action_space.low[0]\n",
    "        high = self.__env.action_space.high[0]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action = self.__policy_phi(torch_state).numpy()\n",
    "            \n",
    "            if noise > 0.0:\n",
    "                action_noise = np.random.normal(scale=noise, size=action.shape[0])\n",
    "                action += action_noise\n",
    "            \n",
    "            action = np.clip(action, low, high)\n",
    "            return action\n",
    "\n",
    "    def update_network(self,\n",
    "                       batch_size: int,\n",
    "                       epoch: int):\n",
    "        \n",
    "        # Sample a batch of transitions from the replay\n",
    "        num_samples, samples = self.__exp_replay.sample(batch_size)\n",
    "        \n",
    "        if num_samples > 0:\n",
    "\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            next_states = []\n",
    "            dones = []\n",
    "            for sample in samples:\n",
    "                s, a, r, n_s, d = sample\n",
    "                states.append(s)\n",
    "                actions.append(a)\n",
    "                rewards.append(r)\n",
    "                next_states.append(n_s)\n",
    "                dones.append(d)\n",
    "\n",
    "            states = torch.Tensor(np.array(states))\n",
    "            actions = torch.Tensor(np.array(actions))\n",
    "            rewards = torch.Tensor(np.array(rewards))\n",
    "            next_states = torch.Tensor(np.array(next_states))\n",
    "            dones = torch.Tensor(1 - np.array(dones))\n",
    "\n",
    "            # Compute Target for Q value update\n",
    "            with torch.no_grad():\n",
    "                Q_s = self.__q_tar(next_states, self.__policy_tar(next_states))\n",
    "                \n",
    "            target = rewards.unsqueeze(dim=1) + self.__gamma * dones.unsqueeze(dim=1) * Q_s\n",
    "            Q = self.__q_phi(states, actions)\n",
    "\n",
    "            self.__tensorboard_writer.add_scalar(\"batch_reward\", rewards.mean(), epoch)            \n",
    "            \n",
    "            q_loss = self.__q_loss_func(Q, target)\n",
    "            self.__q_optimizer.zero_grad()\n",
    "            q_loss.backward()\n",
    "            \n",
    "            # Clip the gradients\n",
    "            torch.nn.utils.clip_grad_norm_(self.__q_phi.parameters(), 1.0)\n",
    "\n",
    "            self.__q_optimizer.step()\n",
    "\n",
    "\n",
    "            if epoch % 500 == 0:\n",
    "                # Write the gradients to TensorBoard\n",
    "                for name, param in self.__q_phi.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        self.__tensorboard_writer.add_histogram(\"critic/\" + name + \"/gradient\", param.grad.data.cpu().numpy(), epoch)\n",
    "\n",
    "            # Update Policy weights\n",
    "            policy_loss = - self.__q_phi(states, self.__policy_phi(states)).mean()\n",
    "\n",
    "            self.__policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "\n",
    "            # Clip the gradients\n",
    "            torch.nn.utils.clip_grad_norm_(self.__policy_phi.parameters(), 1.0)\n",
    "\n",
    "            self.__policy_optimizer.step()\n",
    "\n",
    "            if epoch % 500 == 0:\n",
    "                # Write the gradients to TensorBoard\n",
    "                for name, param in self.__policy_phi.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        self.__tensorboard_writer.add_histogram(\"actor/\" + name + \"/gradient\", param.grad.data.cpu().numpy(), epoch)\n",
    "\n",
    "            # Softly move the weights of target network\n",
    "            for param, target_param in zip(self.__q_phi.parameters(), self.__q_tar.parameters()):\n",
    "                target_param.data.copy_( self.__polyak * target_param.data + (1 - self.__polyak) * param.data )\n",
    "\n",
    "            for param, target_param in zip(self.__policy_phi.parameters(), self.__policy_tar.parameters()):\n",
    "                target_param.data.copy_( self.__polyak * target_param.data + (1 - self.__polyak) * param.data )\n",
    "            \n",
    "            return q_loss.item(), policy_loss.item()\n",
    "\n",
    "\n",
    "    def run_test_episode(self):\n",
    "        state, info = env.reset()\n",
    "        \n",
    "        done = False\n",
    "        cum_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = self.get_action(state,\n",
    "                                     0.0)\n",
    "            # Execute the action\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                done = True\n",
    "            cum_reward += reward\n",
    "            state = next_state\n",
    "        return cum_reward\n",
    "        \n",
    "    def train(self,\n",
    "             train_episodes: int,\n",
    "             noise_scale_start: float,\n",
    "             noise_scale_final: float,\n",
    "             minibatch_size: int,\n",
    "             weight_update_frequency: int,\n",
    "             weight_update_iters: int,\n",
    "             num_test_episodes: int):\n",
    "\n",
    "        assert noise_scale_start > noise_scale_final\n",
    "        \n",
    "        curr_noise = noise_scale_start\n",
    "        noise_decay_constant = np.abs(noise_scale_start - noise_scale_final)/train_episodes\n",
    "\n",
    "        # Num of iterations so far\n",
    "        total_iters = 0\n",
    "        \n",
    "        for episode in tqdm.tqdm(range(0, train_episodes)):\n",
    "            state, info = env.reset()\n",
    "\n",
    "            # Keep track of num of steps in episode\n",
    "            num_steps = 0\n",
    "\n",
    "            # Cummulative reward of the episode\n",
    "            cum_reward = 0\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "\n",
    "                total_iters += 1\n",
    "                \n",
    "                # Get action from Q-network \n",
    "                action = self.get_action(state,\n",
    "                                         curr_noise)\n",
    "\n",
    "                # Execute the action\n",
    "                next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "                cum_reward += reward\n",
    "\n",
    "                # Store the sample in Replay buffer (s, a, r, s',d)\n",
    "                self.__exp_replay.add((state, action, reward, next_state, terminated))\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    done = True\n",
    "\n",
    "                state = next_state\n",
    "                num_steps += 1\n",
    "\n",
    "                \n",
    "                # Update the weights of the network at update frequency\n",
    "                if total_iters % weight_update_frequency == 0 and self.__exp_replay.replay_size > 10000:\n",
    "                    q_losses = []\n",
    "                    policy_losses = []\n",
    "                    \n",
    "                    for _iter_ in range(0, weight_update_iters):\n",
    "                        q_loss, policy_loss = self.update_network(minibatch_size, total_iters)\n",
    "                        q_losses.append(q_loss)\n",
    "                        policy_losses.append(policy_loss)\n",
    "\n",
    "                    q_losses = np.array(q_losses)\n",
    "                    policy_losses = np.array(policy_losses)\n",
    "\n",
    "                    # Log network Losses\n",
    "                    self.__tensorboard_writer.add_scalar('TD Error', q_losses.mean(), total_iters)\n",
    "                    self.__tensorboard_writer.add_scalar('Policy Objective', policy_losses.mean(), total_iters)\n",
    "\n",
    "            # Decrease the noise after every episode\n",
    "            curr_noise -= noise_decay_constant\n",
    "            \n",
    "            # Write the epsiode reward to TensorBoard\n",
    "            self.__tensorboard_writer.add_scalar('Episode Reward', cum_reward, episode)\n",
    "\n",
    "            if episode % 100 == 0:\n",
    "                test_cum_reward = []\n",
    "                for test_episode in range(0, num_test_episodes):\n",
    "                    test_cum_reward.append(self.run_test_episode())\n",
    "                test_cum_reward = np.array(test_cum_reward)\n",
    "                self.__tensorboard_writer.add_scalar('Test Episode Reward', test_cum_reward.mean(), episode)\n",
    "            \n",
    "\n",
    "trainer = Trainer(env,\n",
    "                  hyper_parameters['replay-size'],\n",
    "                  hyper_parameters['hidden_size'],\n",
    "                  hyper_parameters['gamma'],\n",
    "                  hyper_parameters['q_network_lr'],\n",
    "                  hyper_parameters['policy_network_lr'],\n",
    "                  hyper_parameters['polyak'],\n",
    "                  writer)\n",
    "\n",
    "trainer.train(train_episodes=hyper_parameters['num_training_episodes'],\n",
    "              noise_scale_start=hyper_parameters['explore_noise_scale_start'],\n",
    "              noise_scale_final=hyper_parameters['explore_noise_scale_final'],\n",
    "              minibatch_size=hyper_parameters['mini_batch_size'],\n",
    "              weight_update_frequency=hyper_parameters['update_frequency_iters'],\n",
    "              weight_update_iters=hyper_parameters['num_update_iters'],\n",
    "              num_test_episodes=hyper_parameters['num_test_episodes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a0ce4e-310c-4ee5-af17-829c14a70e04",
   "metadata": {},
   "source": [
    "### Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2620b5-4c43-48a1-b27b-76b8c052f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Agent\n",
    "num_test_episodes = 1\n",
    "test_env = env = gym.make(\"BipedalWalker-v3\", hardcore=False, render_mode=\"human\")\n",
    "\n",
    "for episode in range(0, num_test_episodes):\n",
    "\n",
    "    test_env.reset()\n",
    "    # Keep track of num of steps in episode\n",
    "    num_steps = 0\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        test_env.render()\n",
    "        action = test_env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = test_env.step(action)\n",
    "\n",
    "        num_steps += 1\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            done = True\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    print(\"Episodes: {}, Num Steps: {}\".format(episode+1, num_steps))\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b8001d-28f5-4805-868b-ffe8ae0fd37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc7f0c2-0321-49b3-9d22-b741de3ef14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(1000):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "   print(observation.shape)\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
